{
    "evaluate": "def evaluate(self: BaseEnv) -> Dict[str, torch.Tensor]:\n    \"\"\"\n    Return dict is the dict mapping strings of reward-relevant questions to their boolean-valued answers for the batch. So, the values of this dict are torch.Tensors with bool dtype, where the first dimension is a batch of episodes and the second is the boolean answer to the string question for that individual episode.\n\n    Should create a useful set of information. `evaluate` will be called on both the previous state (before the agent took an action) and the current state (after the agent took an action) to calculate reward.\n\n    Always contains a key called \"success\" that maps to the success condition\n    \"\"\"\n    pos_obj = self.obj.pose.p\n    pos_bin2 = self.bin2.pose.p  # Focus on Bin 2 as per feedback\n    offset = pos_obj - pos_bin2\n    xy_flag = torch.linalg.norm(offset[..., :2], axis=1) <= 0.005\n    z_flag = torch.abs(offset[..., 2] - self.radius - self.bin_base_half_len) <= 0.005\n    is_obj_in_bin2 = torch.logical_and(xy_flag, z_flag)  # Check if object is in Bin 2\n    is_obj_grasped = self.agent.is_grasping(self.obj)\n    success = is_obj_in_bin2  # Success is defined as the object being in Bin 2\n    return {\n        'is_obj_grasped': is_obj_grasped,\n        'is_obj_in_bin2': is_obj_in_bin2,\n        'success': success\n    }\n",
    "compute_dense_reward": "def compute_dense_reward(self: BaseEnv, obs: Any, action: torch.Tensor, info: Dict[str, torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    Encodes reward for each possible action based on `evaluate` output dictionary and other current environment info (environment instance attributes). \n\n    Incorporates human feedback as given.\n\n    Obeys the following structure:\n    1. Stage reward \n      - Task is split into stages and reward is given to the agent gradually, encouraging it to complete each stage. The reward accumulates. Each stage's reward is dense wherever possible.\n      - Interdependencies and tradeoffs between different task stages and different aspects of feedback and overall goal are considered. Reward design is not overall counterproductive to meet short-term goals.\n    2. Extra success bump for successful episodes\n    3. Return reward value\n    \"\"\"\n    # Initialize reward tensor\n    reward = torch.zeros(info['success'].shape, device=self.device)\n\n    # Extract relevant positions\n    tcp_pose = self.agent.tcp.pose.p.clone()  # Clone to avoid unintended mutations\n    obj_pos = self.obj.pose.p.clone()\n    bin2_pos = self.bin2.pose.p.clone()\n\n    # Stage 1: Reward for Grasping the Sphere\n    # Encourage the robot to grasp the sphere\n    grasp_reward = info['is_obj_grasped'].float() * 2.0  # Max reward for grasping\n\n    # Stage 2: Reward for Moving Sphere Towards Bin 2\n    # Minimize the distance between the sphere and the top center of Bin 2\n    target_bin2_top = bin2_pos.clone()\n    target_bin2_top[:, 2] += self.bin_base_half_height + self.radius  # Top center position of Bin 2\n    distance_to_bin2 = torch.linalg.norm(target_bin2_top - obj_pos, dim=1)\n    transport_reward = 1.0 - torch.tanh(5.0 * distance_to_bin2)  # Dense reward based on distance\n\n    # Apply transport reward only if the sphere is grasped\n    transport_reward = transport_reward * info['is_obj_grasped'].float()\n\n    # Stage 3: Reward for Positioning Sphere Above Bin 2\n    # Encourage alignment of the sphere above Bin 2\n    above_bin2_flag = distance_to_bin2 <= 0.01  # Threshold for being above Bin 2\n    positioning_reward = above_bin2_flag.float() * 1.0  # Reward for correct positioning\n\n    # Apply positioning reward only if the sphere is grasped and has moved towards Bin 2\n    positioning_reward = positioning_reward * info['is_obj_grasped'].float()\n\n    # Stage 4: Reward for Placing Sphere Inside Bin 2\n    # Encourage the final placement of the sphere inside Bin 2\n    place_reward = info['is_obj_in_bin2'].float() * 5.0  # Significant reward for successful placement\n\n    # Accumulate rewards\n    reward += grasp_reward\n    reward += transport_reward\n    reward += positioning_reward\n    reward += place_reward\n\n    # Extra Success Bump\n    success_boost = info['success'].float() * 10.0  # Additional reward for successful episodes\n    reward += success_boost\n\n    return reward\n"
}