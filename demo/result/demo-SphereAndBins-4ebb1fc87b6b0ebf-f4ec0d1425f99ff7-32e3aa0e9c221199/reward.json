{
    "compute_dense_reward": "def compute_dense_reward(self: BaseEnv, obs: Any, action: torch.Tensor, info: Dict[str, torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    Encodes reward for each possible action based on `evaluate` output dictionary and other current environment info (environment instance attributes). \n\n    Incorporates human feedback as given:\n    - Slow down the robot's movements.\n    - Put the sphere in the other bin (Bin1 instead of Bin2).\n\n    Obeys the following structure:\n    1. Stage reward \n      - Task is split into stages and reward is given to the agent gradually, encouraging it to complete each stage.\n      - Interdependencies and tradeoffs between different task stages and different aspects of feedback and overall goal are considered. Reward design is not overall counterproductive to meet short-term goals.\n    2. Return reward value\n    \"\"\"\n    # Initialize reward tensor with zeros for each environment in the batch\n    reward = torch.zeros(info['success'].shape, device=self.device)\n\n    # Extract relevant positions\n    tcp_pose = self.agent.tcp.pose.p.clone()           # Shape: [num_env, 3]\n    obj_pos = self.obj.pose.p.clone()                  # Shape: [num_env, 3]\n    bin1_pos = self.bin1.pose.p.clone()                # Shape: [num_env, 3]\n\n    # Stage 1: Reward for Grasping the Sphere\n    # Encourage the robot to grasp the sphere\n    grasp_reward = info['is_obj_grasped'].float() * 2.0  # Reward: 2.0 for successfully grasping\n\n    # Stage 2: Reward for Moving Sphere Towards Bin1\n    # Calculate the target position at the top center of Bin1\n    target_bin1_top = bin1_pos.clone()\n    target_bin1_top[:, 2] += self.bin_base_half_height + self.radius  # Adjust z-coordinate for top center\n\n    # Compute the Euclidean distance between the sphere and the target position\n    distance_to_bin1 = torch.linalg.norm(target_bin1_top - obj_pos, dim=1)\n\n    # Dense reward based on proximity to the target position\n    # Closer distance yields higher reward, scaled with a factor for smoothness\n    transport_reward = 1.0 - torch.tanh(5.0 * distance_to_bin1)  # Reward approaches 1 as distance decreases\n\n    # Apply transport reward only if the sphere is grasped\n    transport_reward = transport_reward * info['is_obj_grasped'].float()\n\n    # Stage 3: Reward for Positioning Sphere Above Bin1\n    # Check if the sphere is sufficiently close to be considered above Bin1\n    above_bin1_flag = distance_to_bin1 <= 0.01  # Threshold distance\n    positioning_reward = above_bin1_flag.float() * 1.0  # Reward: 1.0 for correct positioning\n\n    # Apply positioning reward only if the sphere is grasped and has moved towards Bin1\n    positioning_reward = positioning_reward * info['is_obj_grasped'].float()\n\n    # Stage 4: Reward for Placing Sphere Inside Bin1\n    # Encourage the final placement of the sphere inside Bin1\n    place_reward = info.get('is_obj_in_bin1', torch.zeros_like(info['success'])).float() * 5.0  # Reward: 5.0 for successful placement\n\n    # Stage 5: Penalty for Excessive Action Magnitude (Encouraging Slow Movements)\n    # Penalize large actions to encourage smoother and slower movements\n    # Compute the L1 norm of actions for each environment\n    action_penalty = torch.abs(action).sum(dim=1)  # Sum of absolute action values\n    action_penalty = action_penalty * 0.1        # Scaling factor for the penalty\n    penalty_reward = -action_penalty             # Negative reward for large actions\n\n    # Accumulate all stage rewards\n    reward += grasp_reward\n    reward += transport_reward\n    reward += positioning_reward\n    reward += place_reward\n    reward += penalty_reward  # Apply action penalty\n\n    # Extra Success Bump\n    # Provide an additional reward for successfully completing the episode\n    success_boost = info['success'].float() * 10.0  # Reward: 10.0 for successful episodes\n    reward += success_boost\n\n    return reward\n"
}