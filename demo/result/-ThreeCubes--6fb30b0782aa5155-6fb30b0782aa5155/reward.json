{
    "skill_reward": "def skill_reward(self, prev_info, cur_info, action,**kwargs):\n    reward_components = dict((k, 0.0) for k in self.reward_components)\n    current_selected_action = np.argmax(action[:len(self.task_skill_indices.keys())])\n\n    if current_selected_action in [0, 1]:\n        current_selected_pos_1 = action[len(self.task_skill_indices.keys()):len(self.task_skill_indices.keys())+3]\n        current_selected_pos_2 = None\n    else:\n        current_selected_pos_1 = action[len(self.task_skill_indices.keys()):len(self.task_skill_indices.keys())+3]\n        current_selected_pos_2 = action[len(self.task_skill_indices.keys())+3:len(self.task_skill_indices.keys())+6]\n\n    def stage_0_reward():\n        target_action = 0\n        target_pos_1 = prev_info[\"red_cube_pos\"].copy()\n        target_pos_2 = None\n        if current_selected_action==target_action:\n            reward = - np.tanh(np.linalg.norm(current_selected_pos_1-target_pos_1))\n            reward_components[\"afford\"] = (1 + reward) * 5\n        if cur_info[\"stage0_success\"]:\n            reward_components[\"success\"] = 10\n        return reward_components\n    \n    def stage_1_reward():\n        target_action = 1\n        target_pos_1 = prev_info[\"green_cube_pos\"]\n        target_pos_1[2] += 0.04\n        target_pos_2 = None\n        if current_selected_action==target_action:\n            reward = -np.tanh(np.linalg.norm(current_selected_pos_1-target_pos_1))\n            reward_components[\"afford\"] = (1 + reward) * 5\n        if cur_info[\"stage1_success\"]:\n            reward_components[\"success\"] = 10\n        return reward_components\n        \n        \n    def stage_2_reward():\n        target_action = 0\n        target_pos_1 = prev_info[\"purple_cube_pos\"]\n        target_pos_2 = None\n        if current_selected_action == target_action:\n            reward = - np.tanh(np.linalg.norm(current_selected_pos_1 - target_pos_1))\n            reward_components[\"afford\"] = (1 + reward) * 5\n        if cur_info[\"stage2_success\"]:\n            reward_components[\"success\"] = 10\n        return reward_components\n\n    def stage_3_reward():\n        target_action = 1\n        target_pos_1 = prev_info[\"red_cube_pos\"]\n        target_pos_1[2] += 0.04\n        target_pos_2 = None\n        if current_selected_action == target_action:\n            reward = - np.tanh(np.linalg.norm(current_selected_pos_1 - target_pos_1))\n            reward_components[\"afford\"] = (1 + reward) * 5\n        if cur_info[\"stage3_success\"]:\n            reward_components[\"success\"] = 10\n        return reward_components\n\n    if self.cur_stage==0:\n        reward = stage_0_reward()\n    elif self.cur_stage==1:\n        reward = stage_1_reward()\n    elif self.cur_stage==2:\n        reward = stage_2_reward()\n    elif self.cur_stage==3:\n        reward = stage_3_reward()\n    \n    cur_before = self.cur_stage\n    # move to next stage if success\n    if (self.cur_stage == 0) and cur_info[\"stage0_success\"]:\n        self.cur_stage = 1\n    elif (self.cur_stage == 1) and cur_info[\"stage1_success\"]:\n        self.cur_stage = 2\n    elif (self.cur_stage == 2) and cur_info[\"stage2_success\"]:\n        self.cur_stage = 3\n    \n    print(\"stage:\",cur_before, \"->\", self.cur_stage)\n    return reward",
    "evaluate": "def evaluate(self):\n    info= self._get_obs_info()\n    \n    def stage0_success(info):\n        return info[\"is_red_cube_grasped\"]\n    \n    def stage1_success(info):\n        red_not_grasped = ~info[\"is_red_cube_grasped\"]\n        red_on_green = (torch.linalg.norm(info[\"red_cube_pos\"][:2] - info[\"green_cube_pos\"][:2]) < self.cube_size/2) and (info[\"red_cube_pos\"][2] > (info[\"green_cube_pos\"][2] + self.cube_size/2))\n        return (red_on_green and red_not_grasped)\n    \n    def stage2_success(info):\n        return info[\"is_purple_cube_grasped\"]\n    \n    def stage3_success(info):\n        purple_not_grasped = ~info[\"is_purple_cube_grasped\"]\n        purple_on_red = (torch.linalg.norm(info[\"purple_cube_pos\"][:2] - info[\"red_cube_pos\"][:2]) < self.cube_size/2) and (info[\"purple_cube_pos\"][2] > (info[\"red_cube_pos\"][2] + self.cube_size/2))\n        return purple_on_red and purple_not_grasped\n    \n    info[\"stage0_success\"] = stage0_success(info)\n    info[\"stage1_success\"] = stage1_success(info)\n    info[\"stage2_success\"] = stage2_success(info)\n    info[\"stage3_success\"] = stage3_success(info)\n    \n    info[\"success\"] = torch.tensor(False)\n    if self.cur_stage==3:\n        info[\"success\"] = info[\"stage3_success\"]\n    \n    return info"
}