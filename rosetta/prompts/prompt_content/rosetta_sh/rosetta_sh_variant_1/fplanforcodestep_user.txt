# Instructions

You are a reward designer who specializes in creating reward functions to address reinforcement learning challenges. You will produce outlines for two functions, `evaluate` and `compute_dense_reward`, in natural language. They should be specific and ready for coding.

You will receive: 
1. Code for a task environment class that a reinforcement learning-based agent has already been trained in. This includes a reward function the agent was trained on.
2. Feedback provided to the agent by a human who observed it after it had been trained on the existing reward function.

You should:
1. Create a high level strategy for `compute_dense_reward`, and `evaluate` as required. `compute_dense_reward` is the reward function, and `evaluate` is a utility that analyzes the current environment state and compiles information that is provided to `compute_dense_reward`. 
2. Design a phased reward:
  - Divide the task into phases and provide the agent reward incrementally, motivating it to complete each phase. The reward should therefore accumulate, NOT be winner-takes-all at the conclusion.
  - Consider dependencies and trade-offs between different task phases and different elements of the feedback and overall objective. Ensure your reward design isn't detrimental.
3. If there are elements of the feedback that cannot be implemented without modifying other methods, state "I cannot implement <element>" and ONLY implement the other parts of the feedback, if any exist.
4. Describe your reasoning at each step.

Each phase must be a **single result**. It must be a change in an environment state, not a specific action. It must follow one of the following templates: 
- Reward the agent for navigating to <target location>. Example: "reward the agent for navigating to the point immediately left of `cubeA`."
- Reward the agent for moving <target object> to <target location>. Example: "reward the agent for moving the bottle to be inside of the drawer." 
- Reward the agent for <other single result>. Example: "reward the agent for moving more fluidly." 
- Reward the agent for <other single descriptor> <other object>. Example: "reward the agent for aligning the top cube's edges with the bottom cube's edges."
Notes:
- Notice how the phase templates are highly atomic - one single result.
- Note how the phase templates address changes to the environment (including the agent) rather than the agent's actions.

### Original code
```python
$environment_code
```

### Human feedback
Task description PRIOR TO the human providing feedback: $task_description
Agent execution description: $demo_summary
Human's feedback: $grounded_preference

Create a high-level strategy for redesigning `evaluate` and `compute_dense_reward`. The strategy should be a sequence of phases.
- Don't implement yet, just plan the phases.
- Consider dependencies and conflicts between task phases, and develop a strategy that doesn't do anything detrimental.

### RULES YOU MUST FOLLOW
1. Implement human feedback as provided.
2. Do not add anything that opposes the feedback
3. Do not create your own modifications or guess about what the feedback is targeting
