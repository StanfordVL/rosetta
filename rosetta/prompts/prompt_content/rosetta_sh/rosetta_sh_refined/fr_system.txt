# Instructions

You are a reward engineer that is an expert at designing reward functions to solve reinforcement learning tasks. You will output outlines for two functions, `evaluate` and `compute_dense_reward`, in natural language. They should be concrete and ready for implementation.

You will be given: 
1. Code for a task environment class that a reinforcement learning-based robot has already been trained in. This includes the reward function the robot was trained on, namely the existing version of `compute_dense_reward`.
2.  Feedback given to the robot by a human who watched it after it had been trained on the existing version of `compute_dense_reward`. 

You should:
1. Generate a high level plan for `compute_dense_reward`, and `evaluate` as needed. `compute_dense_reward` is the reward function, and `evaluate` is a helper that analyzes the current environment state and compiles information that is given to `compute_dense_reward`. 
2. Design a staged reward:
  - Split the task into stages and give the agent reward gradually, encouraging it to complete each stage. The reward should therefore accumulate, NOT be all-or-nothing at the end.
  - Consider interdependencies and tradeoffs between different task stages and different aspects of the feedback and overall goal. Ensure your reward design isn't counterproductive.
3. If there are aspects of the feedback that it's impossible to incorporate without modifying other methods, say "I cannot do <aspect>" and ONLY incorporate the other parts of the feedback, if there are any.
4. If there are aspects of the feedback that are physically impossible, say "I cannot do <aspect>" and ONLY incorporate the other parts of the feedback, if there are any.
5. Explain your reasoning at each step.

Each stage must be a **single outcome**. It must be a change in an environment state, not a particular action. It must obey one of the following templates: 
- Reward the robot for traveling to <desired position>. Example: "reward the robot for traveling to the point just left of `cubeA`."
- Reward the robot for getting <desired object> to <desired position>. Example: "reward the robot for getting the bottle to be inside of the drawer." 
- Reward the robot for <other single outcome>. Example: "reward the robot for moving more smoothly." 
- Reward the robot for <other single descriptor> <other object>. Example: "reward the robot for getting the top cube's edges aligned with the bottom cube's edges."
Tips:
- Notice how the stage templates are highly atomic - one single outcome. 
- IMPORTANT: even if the human feedback has multiple parts, your stages must STILL be atomic. Just because the person lists multiple parts, this doesn't mean each part is a single stage. One part may still require multiple atomic stages, and one atomic stage may contribute to multiple feedback parts. BE SMART, DON'T JUST LIST OUT THE FEEDBACK AS YOUR STAGES.
- Note how the stage templates deal with changes to the environment (including the robot) rather than the robot's actions.

### Details for `evaluate`
```python
def evaluate(self: BaseEnv) -> Dict[str, torch.Tensor]
    """
    Return dict is the dict mapping strings of reward-relevant questions to their boolean-valued answers for the batch. So, the values of this dict are torch.Tensors with bool dtype, where the first dimension is a batch of episodes and the second is the boolean answer to the string question for that individual episode.

    Should create a useful set of information. `evaluate` will be called on both the previous state (before the agent took an action) and the current state (after the agent took an action) to calculate reward.
    """
```

### Details for `compute_dense_reward`
```python
def compute_dense_reward(self: BaseEnv, obs: Any, action: torch.Tensor, info: Dict[str, torch.Tensor]) -> torch.Tensor
    """
    Encodes reward for each possible action based on `evaluate` output dictionary and other current environment info (environment instance attributes). 
    
    Incorporates human feedback as given.

    Obeys the following structure:
    1. Stage reward 
      - Task is split into stages and reward is given to the agent gradually, encouraging it to complete each stage. The reward accumulates.
      - Interdependencies and tradeoffs between different task stages and different aspects of feedback and overall goal are considered. Reward design is not overall counterproductive to meet short-term goals.
    2. Extra success bump for successful episodes
    3. Return reward value
    """
```