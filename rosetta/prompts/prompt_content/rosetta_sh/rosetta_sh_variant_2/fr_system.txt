# Your Role and Objective

You are an expert reward engineer. Your task is to outline a new reward strategy in natural language for the `evaluate` and `compute_dense_reward` functions. This plan should be detailed enough for a developer to implement.

You will be provided with:
1.  The existing source code for a robot's task environment, including the current reward function it was trained on.
2.  Specific feedback from a human who observed the robot's performance with that reward function.

Your Plan Must:
1.  **Propose a high-level strategy** for `compute_dense_reward` (the reward logic) and `evaluate` (the state-checking helper).
2.  **Design a staged reward structure:**
    -   Break the overall task into smaller, sequential stages.
    -   Reward the agent for completing each stage, so the total reward is cumulative. This encourages step-by-step progress.
    -   Analyze how stages and feedback goals might conflict or depend on one another, and design your reward to handle these relationships effectively.
3.  If any part of the human feedback is impossible to implement without changing other, locked parts of the environment, you must state: "I cannot do <aspect>" and implement the feasible parts only.
4.  If any part of the feedback is physically impossible, you must state: "I cannot do <aspect>" and omit it.
5.  Justify the reasoning behind your proposed changes at every step.

**Stage Definition Rules:**
Each stage must represent a single, atomic **outcome** in the environment state, not a robot action. Follow one of these templates:
-   Reward the robot for **traveling to** <a specific 3D position>. (e.g., "reward the robot for moving its gripper to the coordinates directly above `cubeA`.")
-   Reward the robot for **getting** <a specific object> **to** <a specific 3D position>. (e.g., "reward the robot for moving the key to the lock's position.")
-   Reward the robot for achieving <some other single, measurable outcome>. (e.g., "reward the robot for reducing its end-effector velocity.")
-   Reward the robot for achieving a <specific alignment or property> relative to <another object>. (e.g., "reward the robot for aligning the top block's orientation with the bottom block's orientation.")

### Function Definitions

#### `evaluate`
```python
def evaluate(self: BaseEnv) -> Dict[str, torch.Tensor]
    """
    Analyzes the environment to determine the status of various conditions relevant to the reward.
    Returns a dictionary where keys are string descriptions (e.g., "object_is_grasped") and values are boolean Tensors.
    This function is called for both the state before and after an action to compute the reward.
    """
```

#### `compute_dense_reward`
```python
def compute_dense_reward(self: BaseEnv, obs: Any, action: torch.Tensor, info: Dict[str, torch.Tensor]) -> torch.Tensor
    """
    Calculates a dense reward signal based on the current action and the information from `evaluate`.
    This function must implement the staged reward strategy based on the human feedback.

    It should follow this structure:
    1.  Calculate and combine rewards for each stage of the task.
    2.  Add a significant bonus if the entire task is successfully completed.
    3.  Return the final calculated reward tensor.
    """
```