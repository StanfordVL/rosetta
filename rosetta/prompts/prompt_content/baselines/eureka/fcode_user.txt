We are going to use a Franka Panda robot to complete given tasks. The action space of the robot is a normalized `Box(-1, 1, (num_env,7), float32)`, where `num_env` means the number of environments in parallel, many attributes in the environment are batched, the first dimension is `num_env`.

# Environment code 
$environment_code

# ManiSkill Doc:
$documentation

# Previous Implementations:

Previous reward function:
```python
$compute_dense_reward
```

Your reward function code has been analyzed, the feedback is as follows: 
# Human Feedback:
$original_feedback

# Objective Feedback:
$objective_feedback

Please carefully analyze the policy feedback and provide a new, improved reward function that can better solve the task. Some helpful tips for analyzing the policy feedback:
    (1) If the success rates are always near zero, then you must rewrite the entire reward function

Please generate a reward function that follows all guidelines and addresses the human feedback. The code output should be formatted as a python code string: "```python ... ```".
- "compute_dense_reward": containing the complete reward function code

The functions MUST have these EXACT signatures:

def compute_dense_reward(self: BaseEnv, obs: Any, action: torch.Tensor, info: Dict[str, torch.Tensor]) -> torch.Tensor
    #Encodes reward for each possible action based on `evaluate` output dictionary and other current environment info (environment instance attributes). 
    
    #Incorporates human feedback as given.

    #Obeys the following structure:
    #1. Stage reward 
      - Task is split into stages and reward is given to the agent gradually, encouraging it to complete each stage. The reward accumulates.
      - Interdependencies and tradeoffs between different task stages and different aspects of feedback and overall goal are considered. Reward design is not overall counterproductive to meet short-term goals.
    #2. Return reward value
    # Your implementation here
    pass
```

The function should:
1. Match the exact function signatures shown above
2. Handle batched operations correctly (num_env dimension)
3. Include comprehensive comments
4. Follow the reward function best practices
5. Be consistent with the interface of the previous implementations while incorporating the requested changes

The code output should be formatted as a python code string: "```python ... ```".